# PE_from_scratch
pytorch版本实现的position_embedding,用于学习目的, 帮助更好的了解它的工作原理。

## Position Embedding的背景？

在Transformer这样的模型结构中，自注意力机制本身是置换不变的，这意味着它对输入序列中元素的顺序不敏感，而仅仅关注元素之间的内容关联。然而，对于绝大多数序列任务，无论是自然语言处理中的语句理解，还是时间序列分析，元素的顺序都是至关重要的语义信息。例如，词语在句子中的位置直接影响了句子的含义和语法结构，因此必须引入某种形式的位置信息，使模型能够感知序列的顺序。这就是位置编码被提出的根本原因，它的核心目标是在**不改变自注意力机制基本形式的前提下，为每个序列元素注入位置信息**。

早期的绝对位置编码方法为序列中的每个位置分配一个独立的编码向量，无论是通过正弦余弦函数生成，还是通过学习得到的位置嵌入，这些编码直接与对应位置的词向量相加，从而将绝对位置信息引入模型。这类方法虽然简单有效，但存在明显的局限性：一方面，模型在训练时如果只接触到特定长度的序列，可能难以泛化到更长的序列；另一方面，绝对位置编码更侧重于元素在序列中的绝对“坐标”，而许多语言现象实际上更依赖于元素之间的相对关系，这种关系在不同绝对位置的句子中可能是一致的。因此，研究重点逐渐从绝对位置转向了相对位置编码。

相对位置编码的提出，是为了更直接地建模序列元素之间的距离关系。它不再关注每个位置的绝对编号，而是关注元素两两之间的相对偏移量。一般来说，相对位置编码会引入一个可训练的偏置矩阵，用来表示不同相对距离对注意力得分的影响，这个偏置矩阵是需要训练的，这个矩阵的长宽一般是最大相对距离 * 2 + 1。在自注意力计算中，相对位置信息通常被作为偏置项引入到注意力得分中。。这使得模型能够学习到“距离当前位置k个单位的元素应该如何被关注”的模式，这种模式理论上更容易在不同长度和不同绝对起始位置的序列之间迁移，从而提升了模型的泛化能力和对序列结构的理解。但是，由于引入的矩阵是一个固定大小的查找表，导致模型的泛化能力有限，外推能力有限。

然而，无论是早期的绝对位置编码还是改进的相对位置编码，都面临着一个挑战：如何让模型在无需额外学习的情况下，自然地将位置信息外推到比训练序列更长的场景（外推性）。旋转位置编码ROPE通过将词嵌入向量在复数空间中进行旋转来编码位置信息，旋转的角度与元素的绝对位置成正比。这种设计的精妙之处在于，两个向量Qm和Kn经过旋转后，其内积结果只会依赖于它们之间的相对位置差（m-n），而不是具体的m和n。这就意味着，旋转位置编码在数学形式上等价于同时包含了绝对位置信息（通过旋转角度）和相对位置信息（通过内积结果），并且由于其基于乘法的旋转操作是线性的，使得模型在推理时能够自然地处理远超训练时所见长度的序列，从而具备了强大的外推能力。

## TODO List
- [x] 可训练的绝对位置编码
- [x] 不可训练的SinCos绝对位置编码，与原始Transformer论文一致。
- [x] Swin_Transformer中的相对位置编码
- [x] 旋转位置编码Rope

## Acknowledgements
- [Swin Transformer之相对位置编码详解](https://www.zhihu.com/tardis/zm/art/577855860?source_id=1005)
- [旋转位置编码 (RoPE), 原理与应用](https://vortezwohl.github.io/nlp/2025/05/22/%E8%AF%A6%E8%A7%A3%E6%97%8B%E8%BD%AC%E4%BD%8D%E7%BD%AE%E7%BC%96%E7%A0%81.html#rope-%E7%9A%84%E5%AE%9E%E7%8E%B0-%E5%9F%BA%E4%BA%8E-torch)
- [十分钟读懂旋转编码（RoPE）](https://www.zhihu.com/tardis/bd/art/647109286)
- [Flux中的RoPE](https://github.com/black-forest-labs/flux/blob/main/src/flux/math.py)
- [图解RoPE旋转位置编码及其特性](https://mp.weixin.qq.com/s/-1xVXjoM0imXMC7DKqo-Gw)
